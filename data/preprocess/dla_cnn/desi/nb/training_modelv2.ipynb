{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random, os, sys, traceback, math, json, timeit, gc, multiprocessing, gzip, pickle\n",
    "import peakutils, re, scipy, getopt, argparse, fasteners\n",
    "import glob, sys, gzip, pickle, os, multiprocessing.dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/samwang/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/samwang/Desktop/trial/dla_cnn-master/modell1.py:271: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "from dla_cnn.desi.DesiMock import DesiMock\n",
    "from dla_cnn.desi.model_v2 import build_model\n",
    "from src.Dataset import Dataset\n",
    "from dla_cnn.desi.train_v2 import train_ann_test_batch\n",
    "from dla_cnn.desi.train_v2 import train_ann\n",
    "from dla_cnn.desi.train_v2 import calc_normalized_score\n",
    "from dla_cnn.desi.train_v2 import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG> init dataset file loop, counting samples in [local_train.npy]: 484\n",
      "DEBUG> get_next_buffer called 1 files in set\n",
      "DEBUG> Enter file load loop\n",
      "DEBUG> File load loop complete\n",
      "DEBUG> Enter file load loop\n",
      "DEBUG> File load loop complete\n",
      "DEBUG> init dataset file loop, counting samples in [local_train.npy]: 484\n",
      "DEBUG> get_next_buffer called 1 files in set\n",
      "DEBUG> Enter file load loop\n",
      "DEBUG> File load loop complete\n",
      "DEBUG> Enter file load loop\n",
      "DEBUG> File load loop complete\n"
     ]
    }
   ],
   "source": [
    "#load the npy file to a class Dataset\n",
    "#Dataset in src/Dataset.py ,this code needs to be updated\n",
    "train_datafiles='local_train.npy'\n",
    "test_datafiles='local_test.npy'\n",
    "train_dataset=Dataset(train_datafiles)\n",
    "test_dataset=Dataset(test_datafiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fc2_3_n_neurons': 150, 'pool3_stride': 4, 'conv3_kernel': 16, 'conv1_stride': 3, 'pool1_stride': 4, 'conv3_filters': 96, 'conv2_stride': 1, 'conv1_filters': 100, 'fc2_1_n_neurons': 200, 'dropout_keep_prob': 0.98, 'pool2_kernel': 6, 'pool3_kernel': 6, 'conv2_filters': 96, 'l2_regularization_penalty': 0.005, 'pool2_stride': 4, 'conv2_kernel': 16, 'fc1_n_neurons': 350, 'learning_rate': 2e-05, 'batch_size': 700, 'conv1_kernel': 32, 'training_iters': 1000000, 'fc2_2_n_neurons': 350, 'pool1_kernel': 7, 'conv3_stride': 1, 'INPUT_SIZE': 400}\n"
     ]
    }
   ],
   "source": [
    "#load the hyperparameters\n",
    "#may need to do hyperparameter search\n",
    "import json\n",
    "with open(\"dla_cnn/models/model_gensample_v7.1_hyperparams.json\",'r', encoding='UTF-8') as f:\n",
    "     load_dict = json.load(f)\n",
    "print(load_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Operation 'train_step_ABC_1' type=AssignAddVariableOp>, {})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#build the training model using build_model in model_v2\n",
    "import tensorflow as tf\n",
    "import math\n",
    "from dla_cnn.desi.model_v2 import weight_variable\n",
    "from dla_cnn.desi.model_v2 import bias_variable\n",
    "from dla_cnn.desi.model_v2 import conv1d\n",
    "from dla_cnn.desi.model_v2 import pooling_layer_parameterized\n",
    "from dla_cnn.desi.model_v2 import variable_summaries\n",
    "hyperparameters=load_dict\n",
    "build_model(hyperparameters)\n",
    "#the function variable_summaries and tf.Variable may not work with TF2.1\n",
    "#if so,you can find the replace code among the comments in model_v2(using tf.compat.v1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(sess, save_filename):\n",
    "    if save_filename is not None:\n",
    "        tf.compat.v1.train.Saver().save(sess, save_filename + \".ckpt\")\n",
    "        with open(checkpoint_filename + \"_hyperparams.json\", 'w') as fp:\n",
    "            json.dump(hyperparameters, fp)\n",
    "        print(\"Model saved in file: %s\" % save_filename + \".ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(best_accuracy, last_accuracy, last_objective, best_offset_rmse, last_offset_rmse, best_coldensity_rmse,\n",
    "     last_coldensity_rmse) = train_ann(hyperparameters, train_dataset, test_dataset,\n",
    "            save_filename=checkpoint_filename, load_filename=args['loadmodel'])\n",
    "#this command may strat training,but this will take so long for my computer without GPU,so I run it using the terminal\n",
    "#I successfully run this code with the terminal,but the train.py and Dataset.py need to update\n",
    "#after updating these two codes,the following commands in the terminal will start training\n",
    "#python train.py -r'local_train.npy' -e'local_test.npy'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
